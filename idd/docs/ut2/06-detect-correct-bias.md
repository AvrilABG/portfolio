---
title: "PrÃ¡ctica 6: Detectar y Corregir Sesgo con Fairlearn"
---

## âš™ï¸ Setup del Entorno Completo
```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# Scikit-learn
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, r2_score, mean_squared_error

# Fairlearn - La estrella del show
from fairlearn.metrics import (
    MetricFrame, 
    demographic_parity_difference, 
    equalized_odds_difference,
    selection_rate
)
from fairlearn.reductions import ExponentiatedGradient, DemographicParity

import warnings
warnings.filterwarnings('ignore')

plt.style.use('seaborn-v0_8')
sns.set_palette("husl")
np.random.seed(42)

print("âš–ï¸ PRÃCTICA 6: Detectar y Corregir Sesgo con Fairlearn")
print("ğŸ“Š Parte I: Boston Housing (sesgo racial histÃ³rico)")
print("ğŸš¢ Parte II: Titanic (sesgo gÃ©nero + clase)")
print("ğŸ”§ Parte III: Pipeline automÃ¡tico producciÃ³n")
```
```output
âš–ï¸ PRÃCTICA 6: Detectar y Corregir Sesgo con Fairlearn
ğŸ“Š Parte I: Boston Housing (sesgo racial histÃ³rico)
ğŸš¢ Parte II: Titanic (sesgo gÃ©nero + clase)
ğŸ”§ Parte III: Pipeline automÃ¡tico producciÃ³n
```

# ğŸ“Š PARTE I - BOSTON HOUSING: SESGO RACIAL HISTÃ“RICO
## ğŸ”„ Paso 1: Cargar Boston desde Fuente Original
```python
# Cargar desde fuente original (CMU)
data_url = "http://lib.stat.cmu.edu/datasets/boston"
raw_df = pd.read_csv(data_url, sep="\s+", skiprows=22, header=None)

# Restructurar formato especial
data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])
target = raw_df.values[1::2, 2]

print(f"âœ… Boston Housing cargado: {data.shape}")
```
```output
âœ… Boston Housing cargado: (506, 13)
```

## ğŸ”„ Paso 2: Crear DataFrame con Variable ProblemÃ¡tica
```python
feature_names = ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 
                'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT']

boston_df = pd.DataFrame(data, columns=feature_names)
boston_df['MEDV'] = target

# Decodificar variable B problemÃ¡tica
# B = 1000(Bk - 0.63)Â² â†’ Bk = sqrt(B/1000) + 0.63
boston_df['Bk_racial'] = np.sqrt(boston_df['B'] / 1000) + 0.63

print(f"ğŸš¨ Variable B (racial): correlaciÃ³n = {boston_df['B'].corr(boston_df['MEDV']):.3f}")
print(f"ğŸ“Š ProporciÃ³n racial media: {boston_df['Bk_racial'].mean():.3f}")
```
```output
ğŸš¨ Variable B (racial): correlaciÃ³n = 0.333
ğŸ“Š ProporciÃ³n racial media: 1.216
```

## ğŸ”„ Paso 3: Modelo Baseline Boston (Con Sesgo)
```python
# Preparar features con y sin variable racial
X_with_bias = boston_df.drop(['MEDV', 'Bk_racial'], axis=1)
X_without_bias = X_with_bias.drop(['B'], axis=1)
y_boston = boston_df['MEDV']

# Train modelo con sesgo
X_train, X_test, y_train, y_test = train_test_split(
    X_with_bias, y_boston, test_size=0.3, random_state=42
)

boston_biased_model = LinearRegression()
boston_biased_model.fit(X_train, y_train)
boston_biased_pred = boston_biased_model.predict(X_test)

boston_biased_r2 = r2_score(y_test, boston_biased_pred)
print(f"ğŸ”´ Boston CON sesgo: RÂ² = {boston_biased_r2:.4f}")
```
```output
ğŸ”´ Boston CON sesgo: RÂ² = 0.7112
```

## ğŸ”„ Paso 4: ANÃLISIS PROFUNDO de Sesgo - DetecciÃ³n Sin CorrecciÃ³n
```python
# PASO 4A: Crear grupos por proporciÃ³n racial  
racial_threshold = boston_df['Bk_racial'].median()  # mediana
boston_df['grupo_racial'] = (boston_df['Bk_racial'] > racial_threshold).map({
    True: 'Alta_prop_afroam', 
    False: 'Baja_prop_afroam'
})

print(f"ğŸ‘¥ GRUPOS POR PROPORCIÃ“N RACIAL:")
print(boston_df['grupo_racial'].value_counts())

# PASO 4B: AnÃ¡lisis de distribuciÃ³n de precios por grupo
print(f"\nğŸ’° DISTRIBUCIÃ“N DE PRECIOS POR GRUPO RACIAL:")
price_by_group = boston_df.groupby('grupo_racial')['MEDV'].agg(['mean', 'median', 'std', 'count'])
print(price_by_group)

# PASO 4C: Calcular brecha de precios
price_gap = price_by_group.loc['Baja_prop_afroam', 'mean'] - price_by_group.loc['Alta_prop_afroam', 'mean']
price_gap_pct = (price_gap / price_by_group.loc['Alta_prop_afroam', 'mean']) * 100

print(f"\nğŸš¨ BRECHA DE PRECIOS POR SESGO RACIAL:")
print(f"Diferencia promedio: ${price_gap:.2f}k ({price_gap_pct:.1f}%)")
print(f"Baja prop. afroam: ${price_by_group.loc['Baja_prop_afroam', 'mean']:.2f}k")
print(f"Alta prop. afroam: ${price_by_group.loc['Alta_prop_afroam', 'mean']:.2f}k")

# PASO 4D: Visualizar el sesgo
import matplotlib.pyplot as plt

fig, axes = plt.subplots(1, 2, figsize=(12, 5))

# Histograma de precios por grupo
for group in boston_df['grupo_racial'].unique():
    data = boston_df[boston_df['grupo_racial'] == group]['MEDV']
    axes[0].hist(data, alpha=0.7, label=group, bins=20)
axes[0].set_xlabel('Precio (miles $)')
axes[0].set_ylabel('Frecuencia')
axes[0].set_title('DistribuciÃ³n de Precios por Grupo Racial')
axes[0].legend()

# Boxplot comparativo
boston_df.boxplot(column='MEDV', by='grupo_racial', ax=axes[1])
axes[1].set_title('Precios por Grupo Racial')
axes[1].set_xlabel('Grupo Racial')
axes[1].set_ylabel('Precio (miles $)')

plt.tight_layout()
plt.show()

print(f"ğŸ“Š VISUALIZACIÃ“N: Â¿Se observa sesgo sistemÃ¡tico en las distribuciones?")
```
```output
ğŸ‘¥ GRUPOS POR PROPORCIÃ“N RACIAL:
grupo_racial
Alta_prop_afroam    253
Baja_prop_afroam    253
Name: count, dtype: int64

ğŸ’° DISTRIBUCIÃ“N DE PRECIOS POR GRUPO RACIAL:
                       mean  median        std  count
grupo_racial                                         
Alta_prop_afroam  22.810672    22.0   7.994651    253
Baja_prop_afroam  22.254941    20.4  10.268380    253

ğŸš¨ BRECHA DE PRECIOS POR SESGO RACIAL:
Diferencia promedio: $-0.56k (-2.4%)
Baja prop. afroam: $22.25k
Alta prop. afroam: $22.81k
```
![](../assets/ut2/6-4.png)
```output
ğŸ“Š VISUALIZACIÃ“N: Â¿Se observa sesgo sistemÃ¡tico en las distribuciones?
```

## ğŸ”„ Paso 5: REFLEXIÃ“N Ã‰TICA sobre Variable ProblemÃ¡ticaÂ¶
```python
# PASO 5A: ReflexiÃ³n guiada sobre el uso Ã©tico de variable B
print("âš–ï¸ REFLEXIÃ“N Ã‰TICA SOBRE VARIABLE B")
print("="*50)

print(f"\nğŸ¤” PREGUNTAS PARA REFLEXIONAR:")

print(f"\n1. CONTEXTO HISTÃ“RICO:")
print(f"   - La variable B fue diseÃ±ada en 1978")
print(f"   - Codifica proporciÃ³n de poblaciÃ³n afroamericana") 
print(f"   - CorrelaciÃ³n con precios: {boston_df['B'].corr(boston_df['MEDV']):.3f}")
print(f"   â“ Â¿Es Ã©tico usar esta variable en 2025?")

print(f" No consideramos que saea Ã©tico ya que perpetÃºa sesgo histÃ³rico discriminador")

print(f"\n2. DILEMA DE UTILIDAD:")
print(f"   - La variable B es predictiva (mejora el modelo)")
print(f"   - Pero perpetÃºa sesgos raciales histÃ³ricos")
print(f"   â“ Â¿CuÃ¡ndo la utilidad justifica el sesgo?")

print(f"Tomar en cuenta que la utilidad de la variable no justifica su uso si perpetÃºa discriminaciÃ³n inconcebible")

print(f"\n3. RESPONSABILIDAD PROFESIONAL:")
print(f"   - Sklearn removiÃ³ este dataset por razones Ã©ticas")
print(f"   - Nosotros lo usamos para APRENDER sobre sesgo")
print(f"   â“ Â¿CuÃ¡l es nuestra responsabilidad como data scientists?")

print(f"Estar atentos a los segos de este estilo y no perpetuarlos")

print(f"\n4. ALTERNATIVAS Ã‰TICAS:")
print(f"   - Podemos eliminar la variable B")
print(f"   - Podemos documentar sus limitaciones") 
print(f"   - Podemos buscar proxies menos problemÃ¡ticos")
print(f"   â“ Â¿QuÃ© harÃ­as en un contexto real?")

print(f"QuitarÃ­a la variable B y documentarÃ­a su naturaleza problemÃ¡tica")

# PASO 5B: AnÃ¡lisis de correlaciones alternativas
print(f"\nğŸ“Š ANÃLISIS DE VARIABLES ALTERNATIVAS:")
print(f"Variables que podrÃ­an ser menos problemÃ¡ticas:")

alternative_vars = ['LSTAT', 'RM', 'CRIM', 'TAX', 'PTRATIO']
for var in alternative_vars:
    corr = boston_df[var].corr(boston_df['MEDV'])
    print(f"  {var}: correlaciÃ³n = {corr:.3f}")

print(f"\nğŸ’¡ OBSERVACIÃ“N:")
print(f"Algunas variables tienen correlaciones altas sin sesgo racial explÃ­cito")

# PASO 5C: Marco de decisiÃ³n Ã©tica
print(f"\nğŸ¯ MARCO DE DECISIÃ“N PARA VARIABLE PROBLEMÃTICA:")
print(f"="*50)

print(f"\nâœ… USAR variable B SI:")
print(f"  - Contexto es puramente acadÃ©mico/educativo")
print(f"  - Se documenta explÃ­citamente su naturaleza problemÃ¡tica") 
print(f"  - El objetivo es estudiar/detectar sesgo histÃ³rico")

print(f"\nâŒ NO USAR variable B SI:")
print(f"  - El modelo se usarÃ¡ en producciÃ³n")
print(f"  - AfectarÃ¡ decisiones sobre personas reales")
print(f"  - Existe riesgo de perpetuar discriminaciÃ³n")

print(f"\nâš–ï¸ TU DECISIÃ“N Ã‰TICA:")
print(f"Basado en el anÃ¡lisis, Â¿usarÃ­as la variable B en tu modelo?")
print(f"Â¿Por quÃ©? Â¿QuÃ© consideraciones Ã©ticas son mÃ¡s importantes?")

# PASO 5D: Documentar la decisiÃ³n
boston_ethical_decision = "USAR SOLO PARA EDUCACIÃ“N - NO PARA PRODUCCIÃ“N"
print(f"\nğŸ“‹ DECISIÃ“N DOCUMENTADA: {boston_ethical_decision}")
print(f"ğŸ“ JustificaciÃ³n: La variable B estÃ¡ histÃ³ricamente sesgada. Es Ãºtil para aprender sobre sesgo, pero no debe usarse en modelos reales porque puede perpetuar discriminaciÃ³n racial.")
```
```output
âš–ï¸ REFLEXIÃ“N Ã‰TICA SOBRE VARIABLE B
==================================================

ğŸ¤” PREGUNTAS PARA REFLEXIONAR:

1. CONTEXTO HISTÃ“RICO:
   - La variable B fue diseÃ±ada en 1978
   - Codifica proporciÃ³n de poblaciÃ³n afroamericana
   - CorrelaciÃ³n con precios: 0.333
   â“ Â¿Es Ã©tico usar esta variable en 2025?
 No consideramos que saea Ã©tico ya que perpetÃºa sesgo histÃ³rico discriminador

2. DILEMA DE UTILIDAD:
   - La variable B es predictiva (mejora el modelo)
   - Pero perpetÃºa sesgos raciales histÃ³ricos
   â“ Â¿CuÃ¡ndo la utilidad justifica el sesgo?
Tomar en cuenta que la utilidad de la variable no justifica su uso si perpetÃºa discriminaciÃ³n inconcebible

3. RESPONSABILIDAD PROFESIONAL:
   - Sklearn removiÃ³ este dataset por razones Ã©ticas
   - Nosotros lo usamos para APRENDER sobre sesgo
   â“ Â¿CuÃ¡l es nuestra responsabilidad como data scientists?
Estar atentos a los segos de este estilo y no perpetuarlos

4. ALTERNATIVAS Ã‰TICAS:
   - Podemos eliminar la variable B
   - Podemos documentar sus limitaciones
   - Podemos buscar proxies menos problemÃ¡ticos
   â“ Â¿QuÃ© harÃ­as en un contexto real?
QuitarÃ­a la variable B y documentarÃ­a su naturaleza problemÃ¡tica

ğŸ“Š ANÃLISIS DE VARIABLES ALTERNATIVAS:
Variables que podrÃ­an ser menos problemÃ¡ticas:
  LSTAT: correlaciÃ³n = -0.738
  RM: correlaciÃ³n = 0.695
  CRIM: correlaciÃ³n = -0.388
  TAX: correlaciÃ³n = -0.469
  PTRATIO: correlaciÃ³n = -0.508

ğŸ’¡ OBSERVACIÃ“N:
Algunas variables tienen correlaciones altas sin sesgo racial explÃ­cito

ğŸ¯ MARCO DE DECISIÃ“N PARA VARIABLE PROBLEMÃTICA:
==================================================

âœ… USAR variable B SI:
  - Contexto es puramente acadÃ©mico/educativo
  - Se documenta explÃ­citamente su naturaleza problemÃ¡tica
  - El objetivo es estudiar/detectar sesgo histÃ³rico

âŒ NO USAR variable B SI:
  - El modelo se usarÃ¡ en producciÃ³n
  - AfectarÃ¡ decisiones sobre personas reales
  - Existe riesgo de perpetuar discriminaciÃ³n

âš–ï¸ TU DECISIÃ“N Ã‰TICA:
Basado en el anÃ¡lisis, Â¿usarÃ­as la variable B en tu modelo?
Â¿Por quÃ©? Â¿QuÃ© consideraciones Ã©ticas son mÃ¡s importantes?

ğŸ“‹ DECISIÃ“N DOCUMENTADA: USAR SOLO PARA EDUCACIÃ“N - NO PARA PRODUCCIÃ“N
ğŸ“ JustificaciÃ³n: La variable B estÃ¡ histÃ³ricamente sesgada. Es Ãºtil para aprender sobre sesgo, pero no debe usarse en modelos reales porque puede perpetuar discriminaciÃ³n racial.
```

## ğŸ”„ Paso 6: Cargar y Analizar Titanic
```python
# Cargar Titanic
try:
    titanic = sns.load_dataset('titanic')  # load_dataset
except:
    titanic = pd.read_csv("https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv")

titanic_clean = titanic.dropna(subset=['age', 'embarked'])  # dropna

# AnÃ¡lisis rÃ¡pido de sesgo
gender_survival = titanic_clean.groupby('sex')['survived'].mean()  # mean
class_survival = titanic_clean.groupby('pclass')['survived'].mean()

print(f"ğŸš¨ TITANIC BIAS ANALYSIS:")
print(f"Gender gap: {gender_survival['female'] - gender_survival['male']:.1%}")
print(f"Class gap: {class_survival[1] - class_survival[3]:.1%}")
print("âœ… Ambos tipos de sesgo significativos!")
```
```output
ğŸš¨ TITANIC BIAS ANALYSIS:
Gender gap: 54.8%
Class gap: 41.3%
âœ… Ambos tipos de sesgo significativos!
```
## ğŸ”„ Paso 7: Modelo Baseline Titanic (Con Sesgo)
```python
# Preparar datos Titanic
features_titanic = ['pclass', 'age', 'sibsp', 'parch', 'fare']
X_titanic = titanic_clean[features_titanic].copy()
X_titanic['fare'].fillna(X_titanic['fare'].median(), inplace=True)  # fillna
y_titanic = titanic_clean['survived']
sensitive_titanic = titanic_clean['sex']


X_titanic['fare'].fillna

# Train baseline
X_train_t, X_test_t, y_train_t, y_test_t, A_train_t, A_test_t = train_test_split(
    X_titanic, y_titanic, sensitive_titanic, test_size=0.3, random_state=42, stratify=y_titanic
)

titanic_baseline = RandomForestClassifier(n_estimators=100, random_state=42)
titanic_baseline.fit(X_train_t, y_train_t)
titanic_baseline_pred = titanic_baseline.predict(X_test_t)

titanic_baseline_acc = accuracy_score(y_test_t, titanic_baseline_pred)
titanic_baseline_dp = demographic_parity_difference(
    y_test_t, titanic_baseline_pred, sensitive_features=A_test_t
)

print(f"ğŸ”´ Titanic BASELINE: Accuracy = {titanic_baseline_acc:.3f}")
print(f"ğŸš¨ Demographic Parity Diff: {titanic_baseline_dp:.3f}")
```
```output
ğŸ”´ Titanic BASELINE: Accuracy = 0.673
ğŸš¨ Demographic Parity Diff: 0.113
```

## ğŸ”„ Paso 8: Corregir Sesgo con Fairlearn
```python
# Aplicar ExponentiatedGradient a Titanic
titanic_fair = ExponentiatedGradient(
    RandomForestClassifier(n_estimators=100, random_state=42),
    constraints=DemographicParity()
)

print("ğŸ”§ Aplicando Fairlearn a Titanic...")
titanic_fair.fit(X_train_t, y_train_t, sensitive_features=A_train_t)
titanic_fair_pred = titanic_fair.predict(X_test_t)

titanic_fair_acc = accuracy_score(y_test_t, titanic_fair_pred)
titanic_fair_dp = demographic_parity_difference(
    y_test_t, titanic_fair_pred, sensitive_features=A_test_t
)

print(f"ğŸŸ¢ Titanic FAIR: Accuracy = {titanic_fair_acc:.3f}")
print(f"âš–ï¸ Demographic Parity Diff: {titanic_fair_dp:.3f}")
```
```output
ğŸ”§ Aplicando Fairlearn a Titanic...
ğŸŸ¢ Titanic FAIR: Accuracy = 0.631
âš–ï¸ Demographic Parity Diff: 0.062
```

## ğŸ”„ Paso 9: Trade-off Analysis Titanic
```python
titanic_performance_loss = (titanic_baseline_acc - titanic_fair_acc) / titanic_baseline_acc * 100
titanic_fairness_gain = abs(titanic_baseline_dp) - abs(titanic_fair_dp)

print(f"ğŸ“Š TITANIC TRADE-OFF:")
print(f"Performance loss: {titanic_performance_loss:.1f}%")  
print(f"Fairness gain: {titanic_fairness_gain:.3f}")

if titanic_performance_loss < 5 and titanic_fairness_gain > 0.1:
    titanic_recommendation = "âœ… Usar modelo FAIR - excelente trade-off"
else:
    titanic_recommendation = "âš ï¸ Evaluar caso por caso"

print(f"ğŸ“‹ RecomendaciÃ³n Titanic: {titanic_recommendation}")
```
```output
ğŸ“Š TITANIC TRADE-OFF:
Performance loss: 6.2%
Fairness gain: 0.051
ğŸ“‹ RecomendaciÃ³n Titanic: âš ï¸ Evaluar caso por caso
```